import sys
import os
module_path = os.path.abspath(os.path.join('..'))
sys.path.append(module_path)
import argparse
import json 

import numpy as np
from skimage import measure
import trimesh
import torch
import torch.nn as nn

from models import SurfaceNet, DeepLabv3
from datasets import ScanNet2D3D, get_dataloader
from utils.helpers import print_params, make_intrinsic, adjust_intrinsic
from utils.visualization import create_color_palette
from datasets.scannet.utils_3d import ProjectionHelper
from inference_scenes import _voxel_pixel_association

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")   
print(device)
def matrix_to_marching_cubes(matrix, origin = 0, voxel_size = 0.05, pitch=1.0, semseg_volume = None):
    """
    Convert an (n, m, p) matrix into a mesh, using marching_cubes.
    Parameters
    -----------
    matrix : (n, m, p) bool
      Occupancy array
    color_matrix: (n,m,p,3)
    Returns
    ----------
    mesh : trimesh.Trimesh
      Mesh generated by meshing voxels using
      the marching cubes algorithm in skimage
    """

    rev_matrix = np.logical_not(matrix > 0)  # Takes set about 0.
    # Add in padding so marching cubes can function properly with
    # voxels on edge of AABB

    func = measure.marching_cubes_lewiner

    # Run marching cubes.
    pitch = np.asanyarray(pitch)
    if pitch.size == 1:
        pitch = (pitch,) * 3
    meshed = func(volume=rev_matrix,
                  level=0,  # it is a boolean voxel grid
                  spacing=pitch)

    # allow results from either marching cubes function in skimage
    # binaries available for python 3.3 and 3.4 appear to use the classic
    # method
    vertices, faces, _, _ = meshed
    # Return to the origin, add in the pad_width

    vertex_attributes = {}
    if semseg_volume is not None: 
      verts_ind = np.round(vertices).astype(int)
      semseg = semseg_volume[verts_ind[:,0],verts_ind[:,1],verts_ind[:,2]].astype(np.int32)
      vertex_attributes["semseg"] = semseg
      label_viz = semseg.copy()
      cmap = np.array(create_color_palette()) 
      label_viz[(label_viz<0) | (label_viz>=len(cmap))]=0
      vertex_colors = cmap[label_viz, :]
    else: 
      vertex_colors = None
    vertices = (vertices + origin)*voxel_size
    # create the mesh
    mesh = trimesh.Trimesh(vertices=vertices,
                   faces=faces,vertex_colors= vertex_colors, vertex_attributes= vertex_attributes, process = False)
    return mesh

def main(cfg, args): 

    num_images = 5
    recon_type = args.recon_type
    assert recon_type in ['gt', '2dfeat'], 'Unknown reconstruction type, please choose between gt and 2dfeat.'

    dataset_val = ScanNet2D3D(cfg, split = 'val_scenes_non_overlapping', overfit = cfg["overfit"])
    dataloader_val = get_dataloader(cfg, dataset_val, batch_size= cfg["batch_size"], shuffle= cfg["shuffle_val"], num_workers=cfg["num_workers"], pin_memory= cfg["pin_memory"])

    intrinsic = make_intrinsic(cfg["fx"], cfg["fy"], cfg["mx"], cfg["my"])
    intrinsic = adjust_intrinsic(intrinsic, [cfg["intrinsic_image_width"], cfg["intrinsic_image_height"]], cfg["depth_shape"])

    projector = ProjectionHelper(intrinsic, cfg["proj_depth_min"], cfg["proj_depth_max"], cfg["depth_shape"], cfg["subvol_size"], cfg["voxel_size"]).to(device)
    projector.update_intrinsic(intrinsic)

    if recon_type != 'gt': 
        model_3d  = SurfaceNet(cfg, num_images)
        model_3d.classifier = nn.Sequential(nn.Conv3d(100, cfg["model_3d"]["num_classes"], kernel_size= (1,1,1), padding= 0))    
        print_params(model_3d)
        model_3d.to(device)

        checkpoint3 = torch.load(cfg["model_3d"]["load_path_3d"])
        print("Start checkpoint is: ", checkpoint3["epoch"])
        print("Step of train is: ", checkpoint3["count"])
        print("Step of val is: ", checkpoint3["count_val"])
        model_3d.load_state_dict(checkpoint3["state_dict"])

        model_3d.eval()

        model_2d = DeepLabv3(cfg["model_2d"]["num_classes"])
        model_2d.load_state_dict(torch.load(cfg["model_2d"]["load_path_2d"])["state_dict"])
        model_2d.to(device)
        model_2d.eval()

    out_path = args.output_path
    os.makedirs(out_path)
    mesh_path = os.path.join(out_path, 'mesh')
    semseg_path = os.path.join(out_path, 'semseg_data')
    os.makedirs(mesh_path)
    os.makedirs(semseg_path)
    previous_scan = dataset_val[0]['scan_name']
    subvolume_semseg_list = []
    coordinate_list = []
    #count = 0
    print('Processing', previous_scan)
    for i,sample in enumerate(dataloader_val):
        sample = sample.data
        current_scan = sample['scan_name'][0]

        if recon_type !='gt':
            jump_flag = _voxel_pixel_association(sample, projector)
            if jump_flag:
                print('error in validation batch, skipping the current sample...')
                continue
            with torch.no_grad(): 
                rgb_images = sample['nearest_images']['images'][0].to(device) # [5, 3, 256, 328]
                imageft = model_2d(rgb_images, return_features=True, return_preds=False)
                sample['feat_2d'] = imageft

                preds = model_3d(sample, device) #[N, 41, 32, 32, 64]
                _, preds = torch.max(preds, 1) # preds: [N, 32, 32, 64], 
            preds = preds.squeeze().cpu().numpy() # [32, 32, 64]


        if current_scan == previous_scan: 
            coordinate_list.append(-sample['nearest_images']['world2grid'][0][0][:3, 3])  #list of tensors, each of size (3,)
            if recon_type == 'gt': 
                subvolume_semseg_list.append(sample['label'].squeeze().numpy())
            else: 
                subvolume_semseg_list.append(preds)
        else:  
            volume_coords = torch.stack(coordinate_list) # torch tensor size (n,3), n is number of subvolume inside this scene
            bbox_min = torch.min(volume_coords, 0)[0] # tensor size (3,)
            bbox_max = torch.max(volume_coords, 0)[0] + torch.tensor([32, 32, 64])

            semseg_volume = np.zeros((bbox_max - bbox_min).int().tolist())
            for semseg_subvol, start_ndx in zip(subvolume_semseg_list, coordinate_list):
                start_ndx = (start_ndx - bbox_min).int().tolist()# tensor size (3,)
                semseg_volume[start_ndx[0]: start_ndx[0] + 32, start_ndx[1]: start_ndx[1] + 32, start_ndx[2]: start_ndx[2] + 64] = semseg_subvol

            origin = np.expand_dims(bbox_min.numpy(), 0)
            mesh = matrix_to_marching_cubes(semseg_volume, origin, semseg_volume = semseg_volume)
            np.savez(os.path.join(semseg_path, '%s_attributes.npz'%previous_scan), 
                    **mesh.vertex_attributes)
            mesh.export(os.path.join(mesh_path, previous_scan + '.ply'))
      
            previous_scan = current_scan
            print('Processing', current_scan) 
            subvolume_semseg_list = []
            coordinate_list = []
            coordinate_list.append(-sample['nearest_images']['world2grid'][0][0][:3, 3])  #list of tensors, each of size (3,)
            if recon_type == 'gt': 
                subvolume_semseg_list.append(sample['label'].squeeze().numpy())
            else: 
                subvolume_semseg_list.append(preds)

    volume_coords = torch.stack(coordinate_list) # torch tensor size (n,3), n is number of subvolume inside this scene
    bbox_min = torch.min(volume_coords, 0)[0] # tensor size (3,)
    bbox_max = torch.max(volume_coords, 0)[0] + torch.tensor([32, 32, 64])

    semseg_volume = np.zeros((bbox_max - bbox_min).int().tolist())
    for semseg_subvol, start_ndx in zip(subvolume_semseg_list, coordinate_list):
        start_ndx = (start_ndx - bbox_min).int().tolist()# tensor size (3,)
        semseg_volume[start_ndx[0]: start_ndx[0] + 32, start_ndx[1]: start_ndx[1] + 32, start_ndx[2]: start_ndx[2] + 64] = semseg_subvol

    origin = np.expand_dims(bbox_min.numpy(), 0)
    mesh = matrix_to_marching_cubes(semseg_volume, origin, semseg_volume = semseg_volume)
    np.savez(os.path.join(semseg_path, '%s_attributes.npz'%previous_scan), 
        **mesh.vertex_attributes)
    mesh.export(os.path.join(mesh_path, previous_scan + '.ply'))

if __name__ =='__main__': 
    parser = argparse.ArgumentParser(description='Inference scenes segmentation on validation set')
    parser.add_argument('-c', '--config', default='experiments/cfgs/inference_scenes_segmentation.json',type=str,
                        help='Path to the config file (default: inference_scenes_segmentation.json)')
    parser.add_argument('--output_path', help = 'Path to output folder')
    parser.add_argument('--recon_type', help = 'Type of segmentation, 2 options: gt (for segmenting GT mesh from subvolume) or 2dfeat (for 2d feature input)')
    args = parser.parse_args()
    cfg = json.load(open(args.config))
    main(cfg, args)