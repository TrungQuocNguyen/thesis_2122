from multiprocessing.sharedctypes import Value
import os
import argparse
import json 

import numpy as np
from skimage import measure
import trimesh
import torch

from models import SurfaceNet, create_enet_for_3d, DeepLabv3, ResNeXtUNet
from datasets import ScanNet2D3D, get_dataloader
from utils.helpers import print_params, make_intrinsic, adjust_intrinsic
from datasets.scannet.utils_3d import ProjectionHelper

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")   

def matrix_to_marching_cubes(matrix, origin = 0, voxel_size = 0.05, pitch=1.0, color_matrix = None):
    """
    Convert an (n, m, p) matrix into a mesh, using marching_cubes.
    Parameters
    -----------
    matrix : (n, m, p) bool
      Occupancy array
    color_matrix: (n,m,p,3)
    Returns
    ----------
    mesh : trimesh.Trimesh
      Mesh generated by meshing voxels using
      the marching cubes algorithm in skimage
    """

    rev_matrix = np.logical_not(matrix > 0)  # Takes set about 0.
    # Add in padding so marching cubes can function properly with
    # voxels on edge of AABB
    pad_width = 1
    rev_matrix = np.pad(rev_matrix,
                        pad_width=(pad_width),
                        mode='constant',
                        constant_values=(1))

    func = measure.marching_cubes_lewiner

    # Run marching cubes.
    pitch = np.asanyarray(pitch)
    if pitch.size == 1:
        pitch = (pitch,) * 3
    meshed = func(volume=rev_matrix,
                  level=0.5,  # it is a boolean voxel grid
                  spacing=pitch)

    # allow results from either marching cubes function in skimage
    # binaries available for python 3.3 and 3.4 appear to use the classic
    # method
    vertices, faces, _, _ = meshed
    # Return to the origin, add in the pad_width
    vertices = np.subtract(vertices, pad_width * pitch)
    if color_matrix is not None: 
      verts_ind = np.round(vertices).astype(int)%31
      vertex_colors = color_matrix[verts_ind[:,0],verts_ind[:,1],verts_ind[:,2], :]
    else: 
      vertex_colors = None
    vertices = (vertices + origin)*voxel_size
    # create the mesh
    mesh = trimesh.Trimesh(vertices=vertices,
                   faces=faces,vertex_colors= vertex_colors)
    return mesh


def _voxel_pixel_association(blobs, projection_helper): 
        batch_size = blobs['data'].shape[0]
        proj_mapping = [[projection_helper.compute_projection(d.to(device), c.to(device), t.to(device)) for d, c, t in zip(blobs['nearest_images']['depths'][i], blobs['nearest_images']['poses'][i], blobs['nearest_images']['world2grid'][i])] for i in range(batch_size)]
        blobs['proj_ind_3d'] = []
        blobs['proj_ind_2d'] = []
        jump_flag = False
        for i in range(batch_size):
            if None in proj_mapping[i]: #invalid sample
                jump_flag = True
                break
        if  not jump_flag: 
            for i in range(batch_size):
                proj_mapping0, proj_mapping1 = zip(*proj_mapping[i])
                blobs['proj_ind_3d'].append(torch.stack(proj_mapping0)) # list of [max_num_images, 32*32*64+ 1], total batch_size elements in the list 
                blobs['proj_ind_2d'].append(torch.stack(proj_mapping1)) # list of [max_num_images, 32*32*64 + 1], total batch_size elements in the list      
        return jump_flag


def main(cfg, args): 
    mean = torch.tensor([0.496342, 0.466664, 0.440796]).reshape(1,3,1,1)
    std = torch.tensor([0.277856, 0.28623, 0.291129]).reshape(1,3,1,1)
    num_images = 5

    dataset_val = ScanNet2D3D(cfg, split = 'val_scenes_non_overlapping', overfit = cfg["overfit"])
    dataloader_val = get_dataloader(cfg, dataset_val, batch_size= cfg["batch_size"], shuffle= cfg["shuffle_val"], num_workers=cfg["num_workers"], pin_memory= cfg["pin_memory"])

    intrinsic = make_intrinsic(cfg["fx"], cfg["fy"], cfg["mx"], cfg["my"])
    intrinsic = adjust_intrinsic(intrinsic, [cfg["intrinsic_image_width"], cfg["intrinsic_image_height"]], cfg["depth_shape"])

    projector = ProjectionHelper(intrinsic, cfg["proj_depth_min"], cfg["proj_depth_max"], cfg["depth_shape"], cfg["subvol_size"], cfg["voxel_size"]).to(device)
    projector.update_intrinsic(intrinsic)

    if args.arch_3d == 'surfacenet': 
        model_3d = SurfaceNet(cfg, num_images)
    elif args.arch_3d == 'resnextunet': 
        model_3d = ResNeXtUNet(cfg, num_images)
    else: 
        raise ValueError('3D architecture unknown. Please choose between surfacenet and resnextunet')
    print_params(model_3d)
    model_3d.to(device)

    checkpoint = torch.load(args.checkpoint_3d)
    print("Start checkpoint is: ", checkpoint["epoch"])
    print("Step of train is: ", checkpoint["count"])
    print("Step of val is: ", checkpoint["count_val"])
    model_3d.load_state_dict(checkpoint["state_dict"])

    model_3d.eval()

    if args.arch_2d == 'enet': 
        model_2d_fixed, model_2d_trainable, model_2d_classification = create_enet_for_3d(41, args.checkpoint_2d)
        model_2d_fixed.to(device)
        model_2d_fixed.eval()
        model_2d_trainable.to(device)
        model_2d_trainable.eval()
        model_2d_classification.to(device)
        model_2d_classification.eval()
    elif args.arch_2d == 'deeplabv3': 
        model_2d = DeepLabv3(cfg["model_2d"]["num_classes"])
        model_2d.load_state_dict(torch.load(args.checkpoint_2d)["state_dict"])
        for param in model_2d.parameters():
            param.requires_grad = False
        model_2d.to(device)
        model_2d.eval()
    else:
        raise ValueError('2D architecture unknown. Please choose between enet and deeplabv3') 
    
    out_path = args.output_path
    if not os.path.isdir(out_path):
        os.makedirs(out_path)

    previous_scan = dataset_val[0]['scan_name']
    subvolume_list = []
    coordinate_list = []
    print('Processing', previous_scan)
    for i,sample in enumerate(dataloader_val):
        sample = sample.data
        current_scan = sample['scan_name'][0]

        jump_flag = _voxel_pixel_association(sample, projector)
        #covered_voxels = set()
        #for i in range(5): 
        #    num_ind = sample['proj_ind_3d'][0][i,0]
        #    proj = set(sample['proj_ind_3d'][0][i,1:1+num_ind].tolist())
        #    covered_voxels = covered_voxels.union(proj)
        #if len(covered_voxels) < 500:
        #    print('skipping this sample due to low coverage...') 
        #    continue
        if jump_flag:
            print('error in validation batch, skipping the current sample...')
            continue
        
        with torch.no_grad(): 
            rgb_images = sample['nearest_images']['images'][0].to(device) # [5, 3, 256, 328]
            if args.arch_2d == 'enet': 
                imageft = model_2d_fixed(rgb_images) 
                imageft = model_2d_trainable(imageft) # [5,128, 32, 41]
            elif args.arch_2d == 'deeplabv3': 
                imageft = model_2d(rgb_images, return_features=True, return_preds=False)
            else: 
                raise ValueError('Unknown architecture. Please choose between enet and deeplabv3')
            sample['feat_2d'] = imageft

            preds = model_3d(sample, device) #[N, 2, 32, 32, 64]
            _, preds = torch.max(preds, 1) # preds: [N, 32, 32, 64], 
        if preds.sum().item() == 0: # don't process volume grid with all 0 values. 
            print('Discarding this subvolume because of all 0 values inside')
            continue
        preds = preds.squeeze().cpu().numpy() # [32, 32, 64]
        ####################################################################################
        if current_scan == previous_scan: 
            coordinate_list.append(-sample['nearest_images']['world2grid'][0][0][:3, 3])  #list of tensors, each of size (3,)
            subvolume_list.append(preds)
        
        else:  
            volume_coords = torch.stack(coordinate_list) # torch tensor size (n,3), n is number of subvolume inside this scene
            bbox_min = torch.min(volume_coords, 0)[0] # tensor size (3,)
            bbox_max = torch.max(volume_coords, 0)[0] + torch.tensor([32, 32, 64])

            volume = np.zeros((bbox_max - bbox_min).int().tolist())
            for subvolume, start_ndx in zip(subvolume_list, coordinate_list):
                start_ndx = (start_ndx - bbox_min).int().tolist()# tensor size (3,)
                volume[start_ndx[0]: start_ndx[0] + 32, start_ndx[1]: start_ndx[1] + 32, start_ndx[2]: start_ndx[2] + 64] = subvolume

            origin = np.expand_dims(bbox_min.numpy(), 0)
            mesh = matrix_to_marching_cubes(volume, origin)
            mesh.export(os.path.join(out_path, previous_scan + '.ply'))
            #reset everything for new scene
            previous_scan = current_scan
            print('Processing', current_scan) 
            subvolume_list = []
            coordinate_list = []
            coordinate_list.append(-sample['nearest_images']['world2grid'][0][0][:3, 3])  #list of tensors, each of size (3,)
            subvolume_list.append(preds)
    
    volume_coords = torch.stack(coordinate_list) # torch tensor size (n,3), n is number of subvolume inside this scene
    bbox_min = torch.min(volume_coords, 0)[0] # tensor size (3,)
    bbox_max = torch.max(volume_coords, 0)[0] + torch.tensor([32, 32, 64])

    volume = np.zeros((bbox_max - bbox_min).int().tolist())
    for subvolume, start_ndx in zip(subvolume_list, coordinate_list):
        start_ndx = (start_ndx - bbox_min).int().tolist()# tensor size (3,)
        volume[start_ndx[0]: start_ndx[0] + 32, start_ndx[1]: start_ndx[1] + 32, start_ndx[2]: start_ndx[2] + 64] = subvolume

    origin = np.expand_dims(bbox_min.numpy(), 0)
    mesh = matrix_to_marching_cubes(volume, origin)
    mesh.export(os.path.join(out_path, previous_scan + '.ply'))




if __name__ =='__main__': 
    parser = argparse.ArgumentParser(description='Inference validation scenes')
    parser.add_argument('-c', '--config', default='experiments/cfgs/inference_scenes.json',type=str,
                        help='Path to the config file (default: inference_scenes.json)')
    parser.add_argument('--output_path', help = 'Path to output folder')
    parser.add_argument('--arch_3d', help = 'Architecture of 3D model') # surfacenet
    parser.add_argument('--checkpoint_3d', help = 'Path to 3D model') # '/home/tnguyen/thesis_2122/saved/models/3d_recon_2dfeat_input/10-03_23-13/model_best.pth.tar'
    parser.add_argument('--arch_2d', help = 'Architecture of 2D model') # deeplabv3
    parser.add_argument('--checkpoint_2d', help = 'Path to 2D model')
    args = parser.parse_args()
    cfg = json.load(open(args.config))
    main(cfg, args)